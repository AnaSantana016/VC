{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import math \n",
    "\n",
    "from ultralytics import YOLO\n",
    "import pytesseract\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.94\n",
      "Class name --> car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 320x640 1 car, 223.5ms\n",
      "Speed: 4.7ms preprocess, 223.5ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrícula detectada: iKeFA 5627]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tamaño deseado para mostrar las imágenes\n",
    "desired_size = (400, 300)\n",
    "\n",
    "# Configuración personalizada para pytesseract\n",
    "custom_config = r'--oem 3 --psm 6 outputbase alphanumeric'\n",
    "\n",
    "# Visualización de las imágenes\n",
    "def show_image(window_name, image):\n",
    "    resized_image = cv2.resize(image, desired_size)\n",
    "    cv2.imshow(window_name, resized_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Carga del modelo YOLO\n",
    "model = YOLO('yolov8n.pt')  # Asegúrate de tener la implementación de YOLO cargada\n",
    "\n",
    "# Nombre de las distintas clases\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "# Ruta de la imagen a procesar\n",
    "image_path = \"coche2.jpeg\"\n",
    "\n",
    "# Leer la imagen\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Realizar la detección de objetos con YOLO\n",
    "results = model(img, stream=True)\n",
    "\n",
    "# Variable para indicar si se detectó un coche\n",
    "car_detected = False\n",
    "\n",
    "# Para cada detección\n",
    "for r in results:\n",
    "    boxes = r.boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        # Coordenadas del contenedor\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)  # convertir a valores enteros\n",
    "\n",
    "        # Clase\n",
    "        cls = int(box.cls[0])\n",
    "\n",
    "        # Si la clase detectada es un coche\n",
    "        if classNames[cls] == \"car\":\n",
    "            # Confianza\n",
    "            confidence = math.ceil((box.conf[0] * 100)) / 100\n",
    "            print(\"Confidence --->\", confidence)\n",
    "\n",
    "            print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # Convertir identificador numérico de clase a un color RGB\n",
    "            escala = int((cls / len(classNames)) * 255 * 3)\n",
    "            if escala >= 255 * 2:\n",
    "                R = 255\n",
    "                G = 255\n",
    "                B = escala - 255 * 2\n",
    "            else:\n",
    "                if escala >= 255:\n",
    "                    R = 255\n",
    "                    G = escala - 255\n",
    "                    B = 0\n",
    "                else:\n",
    "                    R = escala\n",
    "                    G = 0\n",
    "                    B = 0\n",
    "\n",
    "            # Dibujar el contenedor y la clase\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "            cv2.putText(img, classNames[cls], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, B), 2)\n",
    "\n",
    "            show_image('Images con clases detectadas', img)\n",
    "\n",
    "            # Si se detecta un coche, actualiza la variable\n",
    "            car_detected = True\n",
    "            # Mostrar solo la región de interés alrededor del coche\n",
    "            car_roi = img[y1:y2, x1:x2]\n",
    "            show_image('Región de interés (coche)', car_roi)\n",
    "\n",
    "# Si se detectó un coche, realiza la detección de matrículas\n",
    "if car_detected:\n",
    "    # Convertimos la imagen a escala de grises\n",
    "    gray = cv2.cvtColor(car_roi, cv2.COLOR_BGR2GRAY)\n",
    "    show_image('Escala de grises', gray)\n",
    "\n",
    "    # Aplicamos un threshold\n",
    "    thresh = cv2.threshold(gray, 140, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "    show_image('Threshold', thresh)\n",
    "\n",
    "    # Buscamos los contornos presentes\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    canvas = np.zeros_like(car_roi)\n",
    "    cv2.drawContours(canvas, contours, -1, (0, 255, 0), 2)\n",
    "    show_image('Bordes', canvas)\n",
    "\n",
    "    # Filtrar contornos en la mitad inferior de la imagen\n",
    "    height, width = car_roi.shape[:2]\n",
    "    lower_half_contours = [cnt for cnt in contours if cv2.boundingRect(cnt)[1] > height // 2]\n",
    "\n",
    "    # Ordenar contornos por área (de mayor a menor)\n",
    "    lower_half_contours = sorted(lower_half_contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Obtener el contorno más grande\n",
    "    if lower_half_contours:\n",
    "        largest_contour = lower_half_contours[0]\n",
    "\n",
    "        # Visualización del contorno más grande\n",
    "        canvas = np.zeros_like(car_roi)\n",
    "        cv2.drawContours(canvas, [largest_contour], -1, (0, 255, 0), 2)\n",
    "        show_image('Contorno más grande en la mitad inferior', canvas)\n",
    "\n",
    "        # Extraer la región correspondiente al contorno más grande\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        region_of_interest = car_roi[y:y + h, x:x + w]\n",
    "\n",
    "        # Mostrar la matrícula\n",
    "        show_image('Matricula', region_of_interest)\n",
    "\n",
    "        # Utilizar pytesseract para la detección de texto en la imagen preprocesada\n",
    "        text = pytesseract.image_to_string(region_of_interest, config=custom_config)\n",
    "        print(\"Matrícula detectada:\", text)\n",
    "\n",
    "else:\n",
    "    print(\"No se detectó un coche en la imagen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde cámara, detección con yolov8 y modelo nano. Visualización propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tamaño deseado para mostrar las imágenes\n",
    "desired_size = (400, 300)  # Puedes ajustar estos valores según tus necesidades\n",
    "\n",
    "# Visualización de las imágenes\n",
    "def show_image(window_name, image):\n",
    "    resized_image = cv2.resize(image, desired_size)\n",
    "    cv2.imshow(window_name, resized_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ruta de la imagen\n",
    "image_path = \"coche2.jpeg\"\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convertirmos la imagen a escala de grises\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "show_image('Escala de grises', gray)\n",
    "\n",
    "# Aplicamos un threshold\n",
    "thresh = cv2.threshold(gray, 140, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "show_image('Threshold', thresh)\n",
    "\n",
    "# Buscamos los contornos presentes\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "canvas = np.zeros_like(img)\n",
    "cv2.drawContours(canvas, contours, -1, (0, 255, 0), 2)\n",
    "show_image('Bordes', canvas)\n",
    "\n",
    "# Filtrar contornos en la mitad inferior de la imagen\n",
    "height, width = img.shape[:2]\n",
    "lower_half_contours = [cnt for cnt in contours if cv2.boundingRect(cnt)[1] > height // 2]\n",
    "\n",
    "# Ordenar contornos por área (de mayor a menor)\n",
    "lower_half_contours = sorted(lower_half_contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# Obtener el contorno más grande\n",
    "if lower_half_contours:\n",
    "    largest_contour = lower_half_contours[0]\n",
    "\n",
    "    # Visualización del contorno más grande\n",
    "    canvas = np.zeros_like(img)\n",
    "    cv2.drawContours(canvas, [largest_contour], -1, (0, 255, 0), 2)\n",
    "    show_image('Largest Contour in Lower Half', canvas)\n",
    "\n",
    "    # Extraer la región correspondiente al contorno más grande\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    region_of_interest = img[y:y + h, x:x + w]\n",
    "\n",
    "    # Visualización de la región de interés\n",
    "    show_image('Region of Interest', region_of_interest)\n",
    "else:\n",
    "    print(\"No se encontraron contornos en la mitad inferior de la imagen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diversos modelos preentrenados, visualizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "C:/Users/otsed/Desktop/RUNNERS_ILUSOS/Multimedia/Bibs/TGC23_PdH_C0056_resultado.mp4 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lenovo\\VC\\Práctica 5\\P5\\VC_P5.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/VC/Pr%C3%A1ctica%205/P5/VC_P5.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Para un vídeo \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/VC/Pr%C3%A1ctica%205/P5/VC_P5.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/otsed/Desktop/RUNNERS_ILUSOS/Multimedia/Bibs/TGC23_PdH_C0056_resultado.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/VC/Pr%C3%A1ctica%205/P5/VC_P5.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m=\u001b[39m model(filename, show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/VC/Pr%C3%A1ctica%205/P5/VC_P5.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\model.py:101\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m'\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 242\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:235\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_model(model)\n\u001b[0;32m    234\u001b[0m \u001b[39m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_source(source \u001b[39mif\u001b[39;49;00m source \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msource)\n\u001b[0;32m    237\u001b[0m \u001b[39m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave_txt:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:213\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz \u001b[39m=\u001b[39m check_imgsz(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mimgsz, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstride, min_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# check image size\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel, \u001b[39m'\u001b[39m\u001b[39mtransforms\u001b[39m\u001b[39m'\u001b[39m, classify_transforms(\n\u001b[0;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz[\u001b[39m0\u001b[39m])) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassify\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m load_inference_source(source\u001b[39m=\u001b[39;49msource,\n\u001b[0;32m    214\u001b[0m                                      imgsz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimgsz,\n\u001b[0;32m    215\u001b[0m                                      vid_stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mvid_stride,\n\u001b[0;32m    216\u001b[0m                                      buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mstream_buffer)\n\u001b[0;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39msource_type\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# streams\u001b[39;00m\n\u001b[0;32m    219\u001b[0m                                           \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m>\u001b[39m \u001b[39m1000\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# images\u001b[39;00m\n\u001b[0;32m    220\u001b[0m                                           \u001b[39many\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m'\u001b[39m\u001b[39mvideo_flag\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mFalse\u001b[39;00m]))):  \u001b[39m# videos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\data\\build.py:172\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    170\u001b[0m     dataset \u001b[39m=\u001b[39m LoadPilAndNumpy(source, imgsz\u001b[39m=\u001b[39mimgsz)\n\u001b[0;32m    171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     dataset \u001b[39m=\u001b[39m LoadImages(source, imgsz\u001b[39m=\u001b[39;49mimgsz, vid_stride\u001b[39m=\u001b[39;49mvid_stride)\n\u001b[0;32m    174\u001b[0m \u001b[39m# Attach source types to the dataset\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39msetattr\u001b[39m(dataset, \u001b[39m'\u001b[39m\u001b[39msource_type\u001b[39m\u001b[39m'\u001b[39m, source_type)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\data\\loaders.py:287\u001b[0m, in \u001b[0;36mLoadImages.__init__\u001b[1;34m(self, path, imgsz, vid_stride)\u001b[0m\n\u001b[0;32m    285\u001b[0m         files\u001b[39m.\u001b[39mappend(\u001b[39mstr\u001b[39m((parent \u001b[39m/\u001b[39m p)\u001b[39m.\u001b[39mabsolute()))  \u001b[39m# files (relative to *.txt file parent)\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    289\u001b[0m images \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m files \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m IMG_FORMATS]\n\u001b[0;32m    290\u001b[0m videos \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m files \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m VID_FORMATS]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: C:/Users/otsed/Desktop/RUNNERS_ILUSOS/Multimedia/Bibs/TGC23_PdH_C0056_resultado.mp4 does not exist"
     ]
    }
   ],
   "source": [
    "# Carga del modelo\n",
    "#model = YOLO('yolov8n.pt') #Contenedores\n",
    "#model = YOLO('yolov8n-seg.pt') #Máscaras\n",
    "model = YOLO('yolov8n-pose.pt')  #Pose\n",
    "\n",
    "#Para un vídeo \n",
    "filename = \"C:/Users/otsed/Desktop/RUNNERS_ILUSOS/Multimedia/Bibs/TGC23_PdH_C0056_resultado.mp4\"\n",
    "results = model(filename, show=True)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intregración con seguimiento (tracking)\n",
    "Nota: he tenido que bajar a la versión de python 3.9.5 e instalar lap con pip install lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo\n",
    "model = YOLO('yolov8n.pt') #Contenedores\n",
    "#model = YOLO('yolov8n-seg.pt') #Máscaras\n",
    "#model = YOLO('yolov8n-pose.pt')  #Pose\n",
    "\n",
    "#Para un vídeo \n",
    "#filename = \"C:/Users/otsed/Desktop/RUNNERS_ILUSOS/Multimedia/Bibs/TGC23_PdH_C0056_resultado.mp4\"\n",
    "filename = \"D:/GH010196s.mp4\"\n",
    "results = model.track(source=filename, show=True)  # BoT-SORT tracker (por defecto)\n",
    "#results = model.track(source=filename, show=True, tracker=\"bytetrack.yaml\")  # ByteTrack tracker\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento de caracteres tras instalar pytesseract y tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesseract\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Previamente debes descargar los ejecutables\n",
    "# Si la ruta de Tesseract no está en el PATH, ruta al ejecutable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract'\n",
    "\n",
    "# Lenguajes disponibles\n",
    "print(pytesseract.get_languages(config=''))\n",
    "\n",
    "#Cargo imagen y ocnvierto a RGB\n",
    "img = cv2.imread('toy.tif') \n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#Aplica reconocedor a imagen cargada\n",
    "print(pytesseract.image_to_string(img_rgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento decaracteres tras instalar easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "#Carga del modelo de lengua\n",
    "reader = easyocr.Reader(['es']) \n",
    "\n",
    "#Reconocimiento de una imagen\n",
    "result = reader.readtext('toy.tif')\n",
    "print(result)\n",
    "\n",
    "#Con restricción de caracteres reconocibles\n",
    "#result = reader.readtext('toy.tif', allowlist ='0123456789')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('yolov7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54711ba1bddc392d48ca20e80feaa9b2e23d43069aa8b98ed16355091034ff6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
